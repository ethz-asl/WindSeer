# training parameters
run:
    plot_every_n_batches: 5
    n_epochs: 1000
    batchsize: 35
    num_workers: 6
    learning_rate_decay: 0.25
    learning_rate_decay_step_size: 750
    compute_validation_loss: True
    compute_validation_loss_every_n_epochs: 10
    custom_init: False
    minibatch_epoch_loss: True
    evaluate_testset: False
    warm_start: False

    # options to store data
    save_model: True
    save_metadata: True
    save_model_every_n_epoch: 25
    save_params_hist_every_n_epoch: 100

    # optimizer settings
    optimizer_type: 'adam'
    optimizer_kwargs: {lr: 1.0E-4}

# loss parameters
loss:
    # Combine any loss components from: ['LPLoss', 'ScaledLoss', 'VelocityGradientLoss','DivergenceFreeLoss', 'KLDivLoss', 'GaussianLogLikelihoodLoss'].
    # Multiple instances of one loss type can be used. Just add numbering behind 'Loss', e.g. 'L1Loss1/2/3'.
    # For 'LPLoss', P can be replaced with the desired order (int>0). If decimal order desired use LPLoss with p in kwargs.
    # 'GaussianLogLikelihoodLoss' can only be used alone.
    loss_components: ['L2Loss']

    # If the homoscedastic uncertainty factors of the losses should be learnt during training. Auto disable if 1 loss comp.
    learn_scaling: True

    # Settings for all of the available loss functions. No need to keep unused ones.
    # loss_factor_init sets the initial value of the learnable homoscedastic factors. If learning is disabled, they will
    # remain at this initial value, otherwise they will be adjusted during learning. No need to set it, if only 1 comp.
    # A loss factor of 0.0 == no scaling, due to the homoscedatic formulation: L += L_q*exp(-q) + q
    L1Loss_kwargs: {exclude_terrain: True, loss_factor_init: -0.5}
    L2Loss_kwargs: {exclude_terrain: True, loss_factor_init: -0.5}
    L4Loss_kwargs: {exclude_terrain: True, loss_factor_init: 1.5}
    LPLoss_kwargs: {exclude_terrain: True, 'p': 1.7, loss_factor_init: 0.0}
    ScaledLoss_kwargs: {exclude_terrain: True, no_scaling: True, max_scale: 4, norm_threshold: 0.5, loss_factor_init: 0.0}
    DivergenceFreeLoss_kwargs: {exclude_terrain: True, loss_type: 'L1', loss_factor_init: -1.5}
    VelocityGradientLoss_kwargs: {exclude_terrain: True, loss_type: 'L1', loss_factor_init: -1.5}
    GaussianLogLikelihoodLoss_kwargs: {exclude_terrain: True, uncertainty_loss_eps: 1e-8, loss_factor_init: 0.0}
    KLDivLoss_kwargs: {loss_factor_init: 0.0}

    # If the individual loss components and their factors should be logged using tensorboard
    log_loss_components: True

    # the weighting function of the loss
    loss_weighting_fn: 0 # 0: off, 1: pressure fluct, 2: pressure grad, 3: vel grad
    loss_weighting_clamp: True

    # If true the labels and predictions are channel-wise scaled by the average of the respective channel
    auto_channel_scaling: True

    # lower limit to the channel scaling values
    eps_scaling: 1
    eps_scheduling_mode: decay
    eps_scheduling_kwargs: {'gamma': 0.998,
                            'step_size': 10}

# dataset parameters
data:
    trainset_name: 'data/train.hdf5'
    validationset_name: 'data/validation.hdf5'
    testset_name: 'data/test.hdf5'

    # channels to load from data, choose from ['terrain', 'ux', 'uy', 'uz', 'turb', 'p', 'epsilon', 'nut']
    input_channels : ['terrain', 'ux', 'uy', 'uz'] # only terrain and velocities must be loaded for EDNN3D
    label_channels : ['ux', 'uy', 'uz']  # velocities are required for EDNN training

    augmentation: True
    augmentation_mode: 1 # 0: subsampling, rotating with no interpolation, 1: subsampling, rotation with interpolation
    augmentation_kwargs: {
        subsampling: True,
        rotating: True
    }
    input_mode: 1
    stride_hor: 1
    stride_vert: 1
    ux_scaling: 1.0
    uy_scaling: 1.0
    uz_scaling: 1.0
    turb_scaling: 1.0
    p_scaling: 1.0
    epsilon_scaling: 1.0
    nut_scaling: 1.0
    terrain_scaling: 1.0
    autoscale: False
    additive_gaussian_noise: True # if true the gaussian noise is additive, else a multiplication of the values
    max_gaussian_noise_std: 0.0 # gaussian noise standard deviation for the normalized wind
    n_turb_fields: 0 # number of turbelence fields
    max_normalized_turb_scale: 0.0 # scale of the turbulent field with respect to normalized wind
    max_normalized_bias_scale: 0.0 # scale of the wind bias with respect to normalized wind
    only_z_velocity_bias: False # If true, the bias is only added to the z-wind
    max_fraction_of_sparse_data: 0.1 # maximum fraction of sampled grid cells for the sparse mask (only relevant for input mode 3 and 4)
    min_fraction_of_sparse_data: 0.001 # minimum fraction of sampled grid cells for the sparse mask (only relevant for input mode 3 and 4)

    trajectory_min_length: 30,
    trajectory_max_length: 300,
    trajectory_min_segment_length: 5,
    trajectory_max_segment_length: 20,
    trajectory_step_size: 1.0,
    trajectory_max_iter: 50,
    trajectory_start_weighting_mode: 0,
    trajectory_lenght_short_focus: False,

# model parameters
model:
    name_prefix: 'test_model'
    model_type: 'ModelEDNN3D'
    model_args: {
        n_x: 64,
        n_y: 64,
        n_z: 64,
        n_downsample_layers: 4,
        n_first_conv_channels: 16,
        channel_multiplier: 2,
        filter_kernel_size: 3,
        interpolation_mode: 'nearest',
        align_corners: False,
        skipping: True,
        use_terrain_mask: True,
        pooling_method: 'striding',
        use_fc_layers: False,
        fc_scaling: 8,
        use_mapping_layer: False,
        potential_flow: False,
        activation_type: 'LeakyReLU',
        activation_args: {negative_slope: 0.1},
        verbose: True,
        submodel_type: 'ModelEDNN3D',
        vae: False,
        use_uz_in: True,
        use_sparse_mask: False,
        use_sparse_convolution: False,

        # uncertainty prediction params
        predict_uncertainty: False,
        uncertainty_train_mode: 'alternating',
        logvar_scaling: 10,

        # uncertainty prediction params
        n_stacked: 3,
        pass_full_output: False,
        submodel_terrain_mask: False
    }